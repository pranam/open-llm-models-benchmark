import ollama
import pandas as pd
import time
import sys
import re
from datasets import load_dataset

# --- Configuration ---
MODELS_TO_BENCHMARK = [
    'tinyllama',
    'qwen2:1.5b',
    'gemma:2b',
    'phi3:mini',
    'mistral:7b',
    'qwen2:7b',
    'llama3:8b',
    'gemma2:9b',
]

# MMLU Configuration
MMLU_SUBSET = 'all'
NUM_QUESTIONS = 100  # Number of questions for MMLU evaluation

# Performance Benchmark Configuration
PERFORMANCE_PROMPT = "The sun is a star, a glowing ball of hot gas. It is the center of our solar system. Write a short, detailed paragraph about the sun's role in supporting life on Earth."

# --- Helper Functions ---
def get_installed_models():
    """Returns a set of model names that are already installed locally."""
    return {model['model'] for model in ollama.list()['models']}

def pull_model(model_name):
    """Pulls a model from the Ollama registry, showing progress."""
    print(f"\nPulling model: {model_name}")
    # ... (code is unchanged)
    current_digest = ''
    try:
        for progress in ollama.pull(model_name, stream=True):
            digest = progress.get('digest', '')
            if digest != current_digest and digest != '':
                current_digest = digest
                print(f"  {digest} {progress.get('status')}")
            if 'total' in progress and 'completed' in progress:
                percent = round((progress['completed'] / progress['total']) * 100)
                bar = 'â–ˆ' * (percent // 2)
                sys.stdout.write(f'\r  Downloading: [{bar:<50}] {percent}% ')
                sys.stdout.flush()
        sys.stdout.write('\n')
        print(f"Successfully pulled {model_name}")
    except Exception as e:
        print(f"Error pulling {model_name}: {e}")
        return False
    return True

# --- Benchmark Functions ---
def benchmark_performance(model_name, prompt):
    """Runs a single model performance benchmark and returns the tokens/sec."""
    print(f"\n--- Benchmarking Performance: {model_name} ---")
    try:
        response = ollama.chat(
            model=model_name,
            messages=[{'role': 'user', 'content': prompt}],
        )
        total_duration = response.get('total_duration', 0)
        eval_count = response.get('eval_count', 0)

        if total_duration > 0:
            tokens_per_second = eval_count / (total_duration / 1e9)
        else:
            tokens_per_second = 0

        print(f"Tokens/sec: {tokens_per_second:.2f}")
        return round(tokens_per_second, 2)
    except Exception as e:
        print(f"Error during performance benchmark for {model_name}: {e}")
        return 0

def load_mmlu_dataset(subset, num_questions):
    """Loads and samples the MMLU dataset."""
    print(f"\nLoading MMLU dataset ({subset} subset)...")
    try:
        dataset = load_dataset("cais/mmlu", subset, split='test', streaming=True)
        sample = list(dataset.shuffle(seed=42).take(num_questions))
        print(f"Loaded {len(sample)} questions from MMLU.")
        return sample
    except Exception as e:
        print(f"Failed to load MMLU dataset: {e}")
        return None

def evaluate_model_on_mmlu(model_name, dataset):
    """Evaluates a model's accuracy on the MMLU dataset sample."""
    print(f"\n--- Evaluating {model_name} on MMLU ---")
    correct = 0
    total = len(dataset)
    choices = ['A', 'B', 'C', 'D']
    
    for i, item in enumerate(dataset):
        sys.stdout.write(f'\r  Question {i+1}/{total}...')
        sys.stdout.flush()
        
        prompt = f"Question: {item['question']}\n"
        for j, choice in enumerate(item['choices']):
            prompt += f"{choices[j]}. {choice}\n"
        prompt += "\nAnswer:"

        correct_answer_letter = choices[item['answer']]
        
        try:
            response = ollama.generate(model=model_name, prompt=prompt)
            response_text = response['response']
            match = re.search(r'\b([A-D])\b', response_text.strip().upper())
            parsed_answer = match.group(1) if match else None
            
            if parsed_answer == correct_answer_letter:
                correct += 1
        except Exception:
            continue

    sys.stdout.write('\n')
    accuracy = (correct / total) * 100 if total > 0 else 0
    print(f"MMLU Accuracy: {accuracy:.2f}%")
    return accuracy

# --- Application Modes ---
def benchmark_mmlu_throughput():
    """Runs the combined MMLU and performance benchmarks."""
    print("Starting Combined LLM Benchmark (Performance + MMLU)...")
    
    mmlu_sample = load_mmlu_dataset(MMLU_SUBSET, NUM_QUESTIONS)
    if not mmlu_sample:
        print("Exiting due to dataset loading failure.")
        return

    installed_models = get_installed_models()
    results = []

    for model_name in MODELS_TO_BENCHMARK:
        if model_name not in installed_models:
            if not pull_model(model_name):
                results.append({'model': model_name, 'error': 'Failed to pull model'})
                continue
        
        accuracy = evaluate_model_on_mmlu(model_name, mmlu_sample)
        tokens_sec = benchmark_performance(model_name, PERFORMANCE_PROMPT)
        
        results.append({
            'model': model_name,
            'mmlu_accuracy_%': round(accuracy, 2),
            'tokens_per_sec': tokens_sec
        })

    df = pd.DataFrame(results)
    print("\n--- Combined Benchmark Results ---")
    
    df = df[['model', 'mmlu_accuracy_%', 'tokens_per_sec']]
    print(df.to_string())

def run_single_prompt(prompt):
    """Runs a single prompt across all benchmark models."""
    print(f'\nRunning prompt: "{prompt}"\n')
    installed_models = get_installed_models()

    for model_name in MODELS_TO_BENCHMARK:
        if model_name not in installed_models:
            print(f"--- Skipping {model_name} (not installed) ---")
            continue

        print(f'--- Response from {model_name} ---')
        try:
            response = ollama.chat(
                model=model_name,
                messages=[{'role': 'user', 'content': prompt}],
            )
            print(response['message']['content'])
        except Exception as e:
            print(f"An error occurred: {e}")
        print("-" * 40)

# --- Main Execution Router ---
def main():
    """Main function to route command-line arguments."""
    if len(sys.argv) < 2:
        print("Usage:")
        print("  python main.py benchmark          - Run the full MMLU and performance benchmark.")
        print("  python main.py single_prompt \"<your_prompt>\" - Run a single prompt across all models.")
        return

    command = sys.argv[1]

    if command == "benchmark":
        benchmark_mmlu_throughput()
    elif command == "single_prompt":
        if len(sys.argv) < 3:
            print("Error: Please provide a prompt for the 'single_prompt' command.")
            print("Usage: python main.py single_prompt \"<your_prompt>\"")
            return
        prompt = sys.argv[2]
        run_single_prompt(prompt)
    else:
        print(f"Error: Unknown command '{command}'")
        print("Usage: See above")

if __name__ == "__main__":
    main()

